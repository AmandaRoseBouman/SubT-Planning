\documentclass{article}

\usepackage{corl_2020} % Use this for the initial submission.
% \usepackage[final]{corl_2020} % Uncomment for the camera-ready ``final'' version.

% IEEE
% \documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper
%
% \IEEEoverridecommandlockouts                             
% \overrideIEEEmargins

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{xcolor}
\usepackage[normalem]{ulem}
\usepackage{subfig}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{todonotes}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{comment}
% \usepackage[noend]{algpseudocode} % this breaks my algorithm box

\usepackage{tikzpagenodes}

% \usepackage[
%   subtle
%   %moderate
% ]{savetrees}

\usepackage{algorithm,algorithmic}

\newcommand{\ph}[1]{{\textbf{#1}:}} % paragraph header
% \newcommand{\todo}[1]{{\color{red} #1 }} % Tasks to do
\newcommand{\note}[1]{{\color{cyan} NOTE: #1 }}
\newcommand{\hn}[1]{{\color{orange} NOTE: (Henry) #1 }}
\newcommand{\gautam}[1]{{\color{cyan}Gautam: #1 }}
% \newcommand{\gautam}[1]{{\color{cyan}}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}

\newcommand{\ali}[1]{{\color{blue} #1 }}
%\newcommand{\ali}[1]{} % to remove Ali's comments



\title{
PLGRIM: Hierarchical Value Learning for Large-scale Coverage in Unknown Environments
}
% Keyword: Large-scale, exploration (coverage in unknown environment)


% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

% NOTE: authors will be visible only in the camera-ready (ie, when using the option 'final'). 
% 	For the initial submission the authors will be anonymized.

\author{
  Sung-Kyun Kim*\\
  NASA Jet Propulsion Laboratory\\
  California Institute of Technology\\
  United States\\
  \texttt{sung.kim@jpl.nasa.gov} \\
  \And
  Amanda Bouman*\\
  Department of Mechanical and Civil Engineering\\
  California Institute of Technology\\
  United States\\
  \texttt{abouman@caltech.edu} \\
  \And
  Gautam Salhotra\\
  Department of Computer Science\\
  University of Southern California\\
  United States\\
  \texttt{salhotra@usc.edu} \\
  \And
  David D. Fan\\
  NASA Jet Propulsion Laboratory\\
  California Institute of Technology\\
  United States\\
  \texttt{david.d.fan@jpl.nasa.gov} \\
  \And
  Kyohei Otsu\\
  NASA Jet Propulsion Laboratory\\
  California Institute of Technology\\
  United States\\
  \texttt{kyohei.otsu@jpl.nasa.gov} \\
  \And
  Joel Burdick\\
  Department of Mechanical and Civil Engineering\\
  California Institute of Technology\\
  United States\\
  \texttt{jwb@robotics.caltech.edu} \\
  \And
  Ali-akbar Agha-mohammadi\\
  NASA Jet Propulsion Laboratory\\
  California Institute of Technology\\
  United States\\
  \texttt{aliagha@jpl.nasa.gov} \\
}


\begin{document}
\maketitle

% IEEE
% % \thispagestyle{empty}
% % \pagestyle{empty}
% \pagestyle{plain}

\begin{abstract}
In order for a mobile robot to achieve higher levels of autonomy in previously unexplored environments, it needs to take into account uncertainties in localization, sensor measurements, and unknown hazards as well as the effect of robot's own actions on these uncertainties. 
%
Making decision in such stochastic setting requires learning value and constructing policies over belief space (space of probability distribution over robot-world state). 
%
Learning value over belief space suffers from computational challenges in large spatial environments (known as curse of dimensionality) and at long temporal horizons (curse of history) required in many real-world missions. %An effective planner must be computationally efficient and handle time constraints without severely impacting the optimality of the solution.
In this work, we propose a scalable value learning method over belief space: PLGRIM (Planning at Local-Global levels with Robust Information roadMaps), that bridges the gap between \textit{(i)} local, high-fidelity information such as traversability, and \textit{(ii)} global, macro-level reward seeking behaviors.
%
By leveraging hierarchical belief space planners with information-rich graph structures PLGRIM can address large-scale coverage problems while providing locally near-optimal plans within a receding horizon.
% We take samples of the belief space in order to construct a graph structure that encodes mission and environment-related information, which we call an Information Roadmap (IRM).  
PLGRIM takes steps in enabling belief space planners %aims at bridging the gap between %pushes the boundaries of the state-of-art in 
on physical robots operating in unknown and complex environments. 
%
As a concrete example, we demonstrate the performance of PLGRIM on Boston Dynamic's legged platform (quadruped SPOT robot) operating in unstructured environments.
%and demonstrate its performance as the planning core of the NeBula 2.0 autonomy framework. NeBula 1.0 (prior to PLGRIM integration) framework and the adopted hardware in this demonstration have contributed to winning first place in the 2020 DARPA Subterranean Challenge, Urban Circuit.

%In  particular,  we  will  discuss  the  behaviors  andcapabilities  which  emerge  from  the  integration  of  the  auton-omy  architecture  NeBula  (Networked  Belief-aware  PerceptualAutonomy)   with   next-generation   mobility   systems.   We   willdiscuss  the  hardware  and  software  challenges,  and  solutionsin mobility, perception, autonomy, and wireless networking, aswell  as  lessons  learned  and  future  directions.  

%our framework for autonomous exploration in simulation with large environments with rough terrains and complex topology.
% We demonstrate our framework in team CoSTAR's participation in the DARPA Subterranean Challenge, in which teams compete to autonomously search for artifacts in large, unknown, GPS-denied environments.
% \gautam{Need to update. possible phrases: active learning, online planning, planning under uncertainty, learning value function in POMDP, MCTS, }
\end{abstract}

% \gautam{PLGRIM (pilgrim): Planning at Local and Global levels with Robust Information Maps}

% Two or three meaningful keywords should be added here
% \keywords{Coverage planning under uncertainty, Hierarchical planning, POMDP} 
\keywords{Stochastic coverage, POMDP, Value learning, Hierarchical planning}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Instructions}

% \ph{Paragraph header} Please start every single paragraph with a paragraph header, summarizing the intention of that paragraph. This is mainly for iterations during the paper preparation. We will remove most of them for the final report.

% {
% \color{orange}
% For the "notation consistency", please take a look at these works (note that the content might NOT be relevant, but notation could be useful)

% \begin{itemize}
% \item Feedback based IRM (for graph abstraction notation):
% FIRM: Sampling-based Feedback Motion Planning Under Motion Uncertainty and Imperfect Measurements (IJRR 2014)

% https://journals.sagepub.com/doi/10.1177/0278364913501564

% \item For 3D grid mapping with "information encoding"
% https://eric-heiden.com/publication/2019-crm-ijrr/

% \item For risk-aware local planning over confidence rich maps:
% https://eric-heiden.com/publication/2017-crm-to-iros/2017-crm-to-iros.pdf
% \end{itemize}


% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Abstract
% 1 Introduction
% 2 Related work
% 3 Problem description/formulation
% -Coverage planning (SubT)
% -POMDP
% 4 Exploration-Coverage Planning Problem
% 5 Overall solution framework
% -Global/local IRM
% -Hierarchical planning
% -System overview
% 6 Algorithm description
% -(POMCP-based coverage planning)
% -Graph-level Planner
% -Lattice-level Planner
% 7 Experimental results
% -Simulation
% -Robot experiments
% 7 Conclusion
% Appendix


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

%\ph{High-level mission}
Consider a large-scale coverage mission in an unknown environment, in which a robot is tasked with exploring and searching a GPS-denied unknown area, under given time constraints. Essential elements of an autonomy architecture needed to realize such a mission include creating a map of the environment, accurately predicting risks, and planning motions that can meet the coverage and time requirements.  In such an architecture, quantifying and planning over uncertainty is essential for creating robust, intelligent, and optimal behaviors.

From value learning perspective, a coverage planning problem in an unknown space can be considered an active learning problem over robot's belief, where belief is defined as the probability distributions over all possible joint robot-world states.
%
The objective is to find the best action sequence that maximizes the accumulated reward over time.  The agent must accumulate data to incrementally build a model of its environment, and must understand the effects of its actions on the quality and quantity of data it collects.

%\ph{Problem description--POMDP perspective}
Since the agent's future actions affect its belief of the world and robot state, this coverage problem is fundamentally a Partially Observable Markov Decision Process (POMDP) problem.
%A POMDP is a principled formalization of 
%a sequential decision making process under motion and sensing uncertainty.
The agent employs the underlying intrinsic model of the sequential action-observation process under uncertainty, so that it can %(asymptotically) converge to the optimal solution in a more 
expand its search structure over the space and learn the value in a more sample-efficient manner than model-free learning approaches.



% \begin{figure}[t]
%   \centering
% %   \includegraphics[width=.48\textwidth]{figures/firstpage_v2.png}
%   \includegraphics[width=.7\textwidth,trim={0 0 0 11.0cm},clip]{figures/firstpage_v2.png}
%   \caption{ %The top figure shows a portion of the course in the Urban Circuit of the DARPA Subterranean Challenge. The blue line highlights two neighboring rooms joined by a narrow passage. CoSTAR efficiently explored both rooms by planning on the IRM. 
%   IRM and lattice visualization during an autonomous exploration mission.}
%   \label{fig:firstPage}
% \end{figure}

% \begin{figure}[t!]
%   \centering
%   \includegraphics[width=.4\textwidth]{IRM_Planning/figures/graph-lattice-map.pdf}
%   \caption{Visualization of hierarchical Information RoadMaps (IRMs).}
%   \label{fig:firstPage}
% \end{figure}
% \begin{figure}[t!]
%   \centering
%   \includegraphics[width=.6\textwidth]{figures/SystemOverview.png}
%   \caption{[WIP] System overview}
%   \label{fig:system_overview}
% \end{figure}
\begin{figure}[ht!]
  \centering
  \subfloat[System diagram\label{fig:system_overview}]{
    \includegraphics[width=.55\textwidth,trim={0 5.5cm 9.0cm 0},clip]{figures/SystemOverview.pdf}
  }
  \quad
%   \subfloat[Visualization of hierarchical IRMs]{%
%   \subfloat[Hierarchical IRMs]{%
  \subfloat[Graph/Lattice IRMs\label{fig:irms}]{%
    \includegraphics[width=.26\textwidth,trim={0 1.65cm 0.0cm 0},clip]{IRM_Planning/figures/graph-lattice-map.pdf}
  }
  \caption{Overall framework of hierarchical Information RoadMaps (IRMs) and POMDP planners.}
  \label{fig:firstPage}
\end{figure}


% \begin{figure}[t!]
%   \centering
%   \includegraphics[width=.6\textwidth,trim={0 0 0 1.0cm},clip]{figures/SystemOverview.png}
%   \caption{System diagram of the overall PLGRIM framework.}
%   \label{fig:system_overview}
% \end{figure}

% \begin{figure}[t!]
%   \centering
%   \includegraphics[width=.6\textwidth]{IRM_Planning/figures/sketch_hierarchical_belief_space.png}
%   \caption{Illustration. [TODO] Visualize the planned path in each level.}
%   \label{fig:illustration}
% \end{figure}

%\ph{Gap in the state-of-the-art}
Belief value learning in POMDP setting suffers from the curse of dimensionality \cite{KLC98} and curse of history \cite{Pineau03}. Many powerful methods are extending the spatial and temporal horizons of POMDPs with varying degrees of efficiency and accuracy (e.g., \cite{silver2010monte,somani2013despot,bonet1998learning,kim2019pomhdp}). In this paper, we deal with exploration problems with very long time horizons (> 1 hour), large spatial extents (> 10 km$^2$), and high dimensional belief states (including beliefs on the state of the environment), that exacerbates the curses of history and dimensionality when planning robot behaviors over the belief space.
%

%\ph{Contributions}
To address this problem, we introduce several key spatial and temporal approximations of the robot policy space to enable computational tractability while constructing an online and real-time solver.
%search space.  This decomposition allows us to approximately solve the optimization problem in a computationally tractable manner.  
Spatially, we decompose the belief space to task-relevant partitions of the space
%into a robot and task-relevant graph structure 
enriched with environment map estimates. %, which reduces our search space for good policies, 
The partitioning structure is called an Information Roadmap (IRM) (Figure \ref{fig:irms}). Temporally, we decompose the problem into a long-range (global) IRM which spans the entirety of the known environment, and a robot-centered short-range (local) IRM. % with fixed size. 
We then propose a receding-horizon-control (RHC)-based solver to address the planning over this hierarchical POMDP structure % problem in a receding horizon fashion, 
in real time.

%\ph{Outline}
Section~\ref{sec:related_work} presents the related work and Section~\ref{sec:ECSPasPOMDP} formalizes the problem.
We propose the hierarchical belief learning and coverage planning framework in Section~\ref{sec:plgrim}. Experimental results in simulation and on physical robots are presented in Section~\ref{sec:exp_results}.% validating our method, and Section~\ref{sec:conclusion} concludes this paper.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}\label{sec:related_work}
%\ph{Coverage--Frontier-based exploration}
Frontier-based exploration is a widely used approach for autonomous exploration (e.g., \cite{yamauchi1997frontier,tao2007motion,keidar2012robot,heng2015efficient,gonzalez2002navigation,grabowski2003autonomous}). By continuing exploration until exhausting all remaining frontiers, frontier-based approaches can guarantee \textit{completeness} of the coverage of reachable spaces.  These methods typically rely on myopic (e.g., one-step) look-ahead greedy policies, selecting the best frontier upfront. Hence they can be subject to local minima and provide suboptimal solutions in time.

%\ph{Coverage--(Model-free) RL-based approaches}
Model-free reinforcement learning (RL) has been applied to coverage and exploration problems (e.g., \cite{pathak_icm, rnd,burda2018study,ECR2018}). In this setting, the typical approach is to find a policy which maps sensor data to actions, to maximize the reward. When it comes to long-range and large-scale, and (possibly safety-critical) missions on physical robots, collecting necessary data for this class of methods can be a significant challenge.

%\ph{Coverage--(Model-based RL) POMDP approaches}
POMDP-based approaches generate a non-myopic policy by considering long-horizon action sequences, interactively learning the value function of each ??, and returning the best action sequence that maximizes the accumulated rewards. Different methods have reduced the complexity of the POMDP problem in coverage and exploration problems. \citet{indelman2015planning} and \citet{martinez2009bayesian} employed a direct policy search scheme with a Gaussian belief assumption.  \citet{Lauri2016planning} extended this to non-Gaussian beliefs based on the POMCP (Partially Observable Monte-Carlo Planning). % algorithm that uses a Monte-Carlo Tree Search \cite{silver2010monte}.
In this work, we aim at scaling the solution even further to enable solutions for hte missions fo interste that are longer and larger than mission even with such techniques for approximation the POMDP-based approach does not scale well due to the curse of history and dimensionality \cite{Pineau03}.

\ph{Large scale--Hierarchical approaches}
Hierarchical planning structures aim to tackle larger problems by employing multiple solvers running at different resolutions.  
%
In the coverage and exploration context, \citet{umari2017autonomous} applied hierarchical planning to frontier-based exploration, while  \cite{dang2019explore} extended the lower level module to a more sophisticated frontier selection algorithm considering the information gain along each path.  \citet{Lauri2016planning} replaced the lower level module with a POMDP-based planner, to improve local coverage performance with non-myopic planning.
In the hierarchical POMDP context, \citet{vien2015hierarchical} proposed a hierarchical POMCP framework which outperformed Bayesian model-based hierarchical RL approaches in some benchmarks.  Finally, \cite{kim2019bi} proposed a hierarchical online-offline POMDP solver for risk-aware navigation planning.

\section{Exploratory Covering Salesman POMDP}
\label{sec:ECSPasPOMDP}
We first briefly introduce the coverage problem as a variant of the traveling salesman problem, and show how we formalize it as a POMDP. We then discuss why a naive solution approach to this problem can easily become intractable.  

\ph{Covering Salesman Problem (CSP)} Given a known environment represented by an abstract graph structure $W = (V, E)$, with free and occupied nodes $V_{free}\cup V_{occ} = V$, the traditional coverage problem aims to find a sequence of nodes and edge traversals that pass through all free nodes $V_{free} \subseteq V$.  By incorporating a sensor field-of-view $F:V\rightarrow \mathcal{P}(V)$, which maps each node to a subset of "viewable" nodes, and adding the objective of minimizing travel distance, the coverage problem can be seen as a generalization of the traveling salesman problem -- the \emph{covering salesman problem} (CSP).  In the CSP, the objective is to determine the minimum length path through a subset of nodes $\{v_i\}_i \subseteq V_{free}$ such that every free node is within the accumulated sensor field-of-view: $V_{free} \subseteq \cup_i F(v_i)$.
In this paper, we further modify the CSP by assuming that the environment $W$ is not known a priori, which we call an \emph{exploratory covering salesman problem} (ECSP).  After incorporating a consideration of motion and sensing uncertainty of the agent, we can cast the ECSP as a POMDP problem with a reward function designed for coverage, which we will refer to as \emph{ECSP-POMDP} hereafter.

\ph{ECSP-POMDP Structure} A POMDP is described as a tuple $\langle \mathbb{S}, \mathbb{A}, \mathbb{Z}, T, O, R \rangle$, where $\mathbb{S}$ is the set of states of the agent and world, $\mathbb{A}$ is the set of actions, and $\mathbb{Z}$ is the set of observations sensed by the agent of the world. At every time step, the agent performs an action $a \in \mathbb{A}$ in state $s$ and receives an observation $z \in \mathbb{Z}$ resulting from the agent's perceptual interaction with the environment. The motion model $T(s, a, s') = P(s'\,|\,s, a)$ defines the probability of being at state $s'$ after taking an action $a$ in state $s$. The observation model $O(s, a, z) = P(z\,|\,s, a)$ is the probability of receiving observation $z$ after taking action $a$ in state $s$. Finally, the reward function $R(s,a)$ returns the expected utility for executing action $a$ in state $s$.

\ph{State} We define the system (robot-world) state as a 2-tuple $s = (W, Q) \in \mathbb{W}\times\mathbb{Q} =  \mathbb{S}$ consisting of the world representation $W$ and the robot state $Q$. The world representation can be further decomposed as $W = (W_{occ}, W_{cov})$ where $W_{occ}$ describes a representation of the geometry of the world and $W_{cov}$ encodes which regions of the world have been observed by the robot.  For example, $W_{occ}:\mathbb{Q}\rightarrow\{0,1\}$ can be a mapping from pose to the set $\{0,1\}$, where $W_{occ}(Q) = 0$ if the location is free and $W_{occ}(Q) = 1$ if the location is occupied (and similarly for $W_{cov}$).  We also denote the ground truth occupancy of the world as $W_{occ}^{GT}$.

\ph{Belief State} Since the state of the world is not fully observable, the agent maintains a belief state $b_t\in \mathbb{B}$ defined as the posterior distribution over all possible states conditioned on past actions and observations at time $t$. The belief over state $s$ is $b_{t} = P(s \,|\, a_{0:t-1}, z_{1:t})$. Our belief state is a 2-tuple $b_t = p(s_t) = p(W,Q)$ consisting of the partially observed world $W$ and the fully observed robot state $Q$.


\ph{Transition Model} Given an action $a$, the estimated transition model $T$ deterministically maps the robot and world state $s = (W, Q)$ to the subsequent state $s' = (W', Q')$:
\begin{align}
    T(s' | s, a; \, W_{occ}^{GT}): \, \mathbb{S} \times \mathbb{A} \rightarrow \mathbb{S}
\end{align}
The world occupancy information encodes the traversability risk of an action $a \in \mathbb{A}$. If the traversability risk exceeds a pre-defined threshold, the robot does not transition along action-associated edge $e$ and, consequently, the state does not change: $s = s'$.

\ph{Observation} Upon taking an action $a$ in state $s$, the robot receives a full observation of the robot state $Q$ and a partial observation of the world state $W$.  Given a robot state $Q$, the robot observes areas of the world within the field-of-view of its sensors.  The observation model is defined as:
\begin{align}
   O(z | s, a; \, W_{occ}^{GT}): \, \mathbb{S} \times \mathbb{A} \rightarrow \mathbb{Z}
\end{align}
where $O$ provides information about $W_{cov}$ within the field-of view, which depends on a ray-casting model.

\ph{Traditional Map Representation} A commonly used approximation to the world state $W$ (which we introduced as having a continuous domain, i.e. $W:\mathbb{Q}\rightarrow \{0,1\}$) is the use of a grid map.  By using grid cells $W\approx\bar{W}=\{m_i\}_i$ where $m_i$ represents the world state (occupancy, coverage, etc.) at the $i$-th cell, the world state is reduced to a discrete domain.  In this work, we lightly assume the presence of a mapping and costmap-generating module which creates a local map $\bar{W}$.  
In order to compute the posterior probability $p(\bar{W} | z_{1:t}, Q_{1:t})$ of the whole map $\bar{W}$, given the robot's measurements $z_{1:t}$ and trajectory $Q_{1:t}$, the binary cell states are traditionally approximated by assuming full independence between them \cite{TBF05,elfes1990stochastic}.  Note that to be useful, the resolution of $\bar{W}$ may need to be very high, in the range of 10cm for a 1m-sized robot.  

\ph{Pose Graph} In addition to a map representation of the environment, we employ a \textit{pose graph}, where $\mathcal{PG} = p(Q_{0:t})$ is the belief over the path history taken so far.  This pose graph is generated by a SLAM algorithm and should run in realtime to support our planning architecture.  In practice we may simplify our representation of $\mathcal{PG}$ by approximating it with the maximum likelihood estimate.

\ph{Reward} The one-step estimated reward is computed as a function of the information gain and cost associated with an action:
\begin{align}
    {R}(s, a) = \mathcal{F}\Big[\, \textit{InfoGain}(W_{cov}, z), \; \textit{Cost}(W_{occ}, a) \, \Big]: \, \mathbb{S} \times \mathbb{A} \rightarrow \mathbb{R} 
\end{align}
Information gain is defined as the marginal gain of information: $\textit{InfoGain}(W_{cov}, z) = I(W_{cov} \cup z) - I(W_{cov})$, since the benefit of being in a state $s$ is dependent on whether the robot has already observed neighboring areas in the world. The cost of an action $Cost$ is a function of the world geometry $W_{occ}$ since this sub-state informs both the required actuator output (path length and velocity) and the proximity to the actuator's limitations (traversability risk) required to execute an action.

\ph{ESCP-POMDP} We define a belief policy as a function $\pi : \mathbb{B} \rightarrow \mathbb{A}$ which maps each belief state $b$ to an action $a$.  Let the belief reward $r(b,a)=\int_s R(s,a)b(s)ds$ be the expected reward given a belief.  Then the optimal policy maximizes the expected discounted sum of future belief rewards.
\begin{align}
  \pi^*(b) &= \arg\max_\pi \, \mathbb{E} \left[ \sum_{t=0}^{L} \gamma^t r(b_t, \pi(b_t)) \right]
  \label{eq:optimal_policy}
\end{align}
\todo{add constraints to equation}
where $\gamma \in (0,1)$ is a discount factor which ensures that immediate rewards have a greater effect on decisions than future rewards. The overall objective is to solve this optimization problem in a computationally tractable way.

\ph{Intractability} 
With these classic approximations to the belief of the world state and robot state, solving Eq. (\ref{eq:optimal_policy}) is still highly intractable for large problems.  This is because of 1) the high dimension of the belief state $b$, particularly with respect to the belief over the map $\bar{W}$, and 2) the large timescale $L$, since the complexity of the search space grows exponentially with time.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{PLGRIM: Hierarchical Coverage Planning on Information Roadmaps}
\label{sec:plgrim}

\ph{Our Contributions} 
To obtain a tractable solution to the POMDP problem described above, we propose a novel approach in which we 1) introduce a hierarchical approximation of the belief space $b=p(W,Q)$ to reduce the policy search space, and 2) introduce a receding horizon hierarchical POMCP solver to quickly find good solutions on local to global scales in real-time.

\ph{Architecture}  We decompose the problem into tractable subproblems by introducing spatial and temporal discretizations which enable efficient and reactive robot behaviors on very large scales (kms).  The hierarchical planner has two cascaded layers with different environment representation scales. The higher layer, which we call a \emph{graph-level planner}, employs a sparse graph structure of dynamic size which captures the connectivity of the free space in the known part of the environment. The lower layer, which we call a \emph{lattice-level planner}, employs an agent-centered dense grid structure of fixed dimensions that moves with the robot (Figure \ref{fig:system_overview}). %6

\subsection{Hierarchical Belief Space Representation} 

\ph{Belief Encoding} Both the graph-level planner and lattice-level planner use a graph-based approximation of the world state $W$.  We call these graph-based approximations an \textit{Information Roadmap (IRM)} \cite{agha2014firm}.  We use super-scripts $(\cdot)^g$ or $(\cdot)^l$ as needed to describe objects specific to the graph-level or lattice-level respectively, and describe the graph-level IRM first, but note that the same method applies to the lattice-level IRM.  Let $V^g=\{v_i\}_i$ be a set of poses $v_i\in \mathbb{Q}$.  Then for each pose $v_i\in V^g$ we construct an approximation of $W(v_i) \approx W^g(v_i)$, with $W_{occ}^g(v_i) = \{0,1\}$ and $W_{cov}^g(v_i) = \{0,1\}$.  This approximation can be constructed by finding the maximum value $W_{occ}(v_j)$ for all $v_j$ in a neighborhood of $v_i$, or some other approximate method.  We also can approximate $W_{cov}^g$ as a function of the poses $V^g$ and the pose graph $\mathcal{PG}$.  For details on computing these approximations for both graph-level and lattice-level, see the supplemental material.  We then construct an approximate graph-level belief $b^g=p(W^g, Q)$.  By assuming independence between variables, this belief can be decomposed as $b^g=p(Q)\prod_i p(W_{occ}^g(v_i))$.  Additionally, to create a meaninful decomposition of the action space, we construct edges $E^g=\{e_{ij}\}_{i,j}$ which connect the nodes $v_i, v_j$.  In doing so we create a graph structure $G^g=(V^g, E^g)$ which is a graph-based representation of the world.  We associate to the edges transition probabilities as well as other quantities of interest (e.g. risk, cost of traverse, etc).

Fig. \ref{fig:system_overview} illustrates the process by which the 3D volumetric information $W$, acquired from raw sensor data, is approximated into discrete probability distributions over occupancy and coverage. 
Therefore, we have approximated and discretized the belief space $b^g\in\mathbb{B}^g\subset\mathbb{B}$ and $b^l\in\mathbb{B}^l\subset\mathbb{B}$ for the graph and lattice-level planners act on.

\ph{POMDP Specifics} For each planning problem in the hierarchy we define a sub-POMDP problem over the graph representation.  The details of these problems differ in terms of the definitions for transition models $T^g(\cdot),T^l(\cdot)$, observations $O^g(\cdot),O^l(\cdot)$, and rewards $R^g(\cdot),R^l(\cdot)$.  We detail the particular design choices used in this work in the supplementary material, and give the general framework here.

\subsection{Hierarchical Policy Formulation}
\label{sec:hierarchical_policy}
\ph{Policy} We define a graph-level policy as a mapping from the graph-level belief state $b^g$ to a goal-related parameter $\theta^g \in \Theta^g$ 
\begin{align}
    \pi^g : \mathbb{B}^g \to \mathbb{A}^g, \; \mathbb{A}^g = \Theta^g
\end{align}
The action space $\Theta^g$ encodes a global, non-myopic task, and serves as an input to the lattice-level policy: 
\begin{align}
    \pi^l: \mathbb{B}^l \times \Theta^g \to \mathbb{A}^l
\end{align}
The output of the lattice-level policy is a controller to move the robot (e.g. a waypoint or path for a controller to track, or parameters for a feedback policy that generates control commands).

We define an overall policy $\pi \in \Pi$ generated by combining the graph-level policy $\pi^g$ and the lattice-level policy $\pi^l$:
\begin{align}
    \pi(b) = \pi^l(b^l; \, \pi^g(b^g)) = \pi^l(b^l; \, \theta^g) : \, \mathbb{B}\rightarrow \mathbb{A} 
\end{align}

\ph{Reward} The one-step reward received by the agent upon taking a lattice-level policy $a^l$ in belief $b^l$ is:
\begin{align}
    r^l(b^l, a^l) = r^l(b^l, \pi^l(b^l; \theta^g) = r^l(b^l, \pi^l(b^l; \pi^g(b^g)))
\end{align}

\ph{Optimal policy/ global optimization objective} We then define the optimal policy of the unified composition of the graph and lattice level policies as:
\begin{align}
  \pi^{*}(b) &= \arg\max_\pi \, \mathbb{E} \left[ \sum_{t=0}^{L} \gamma^t r^l(b^l_t, \pi^l(b^l_t; \pi^g(b^g_t)) \right]
  \label{eq:optimal_policy_unified}
\end{align}

\ph{Policy decomposition and interaction} In this work we choose to represent the graph planner's output actions $\theta^g$ as nodes $v^g_i \in V^g$ in the graph-level IRM $G^g$. These nodes are passed to the lattice planner, whose objective is to explore locally. Similarly, the lattice-level planner's output actions are nodes $v^l_i \in V^l$ in the lattice-level IRM $G^l$.  In order to maintain a tractable problem in both the graph and lattice levels, we decouple the policies in the following manner:  The lattice-level planner typically sends its action $v^l_i$ to the kinodynamic planner, which produces velocity commands for the robot.  However, if there is no exploration possible in the lattice map, the lattice-level planner passes along the graph-level planner's action $v^g_i$ to the kinodynamic planner, which encodes a new area in the graph-level IRM for the lattice-level planner to explore.  At any time the lattice-level planner may decide to continue exploring on the lattice-level IRM.  In this manner, we encourage high-fidelity local exploration without sacrificing large-scale, globally optimal decision making.

In the next section we describe the POMDP solver approach to optimizing the global and local policies of equation \ref{eq:optimal_policy_unified}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Real-time Hierarchical Solver for ECSP-POMDPs}

\input{IRM_Planning/algo_pomcp.tex}

\begin{figure}[t!]
  \centering
%   \includegraphics[width=1.0\textwidth]{figures/belief_tree_search.pdf} % without an arrow
  \includegraphics[width=1.0\textwidth]{figures/belief_tree_search_policy.pdf} % with an arrow
  \caption{Illustration of coverage planning with Monte-Carlo Tree Search. A state contains a coverage state of the environment $W$ and a robot pose $q$, and the current state is $(W_o, q_2)$. From forward simulation of possible action sequences and back-propagation of the rewards (e.g., information gain per traversal distance), it can find the best action for the current state, which is \texttt{move-to-$q_1$}.}
  \label{fig:belief-tree-search}
\end{figure}



\ph{Architecture} Algorithm~\ref{alg:hierarchicalPlanner} outlines the overall PLGRIM framework for hierarchical coverage planning.  (See also Figure \ref{fig:system_overview}.)
At each replanning episode, both the graph-level and lattice-level planners employ a POMDP solver to find a policy at each level, $\pi^g$ and $\pi^l$.  To find these policies we employ POMCP, a Monte-Carlo Tree Search-based POMDP solver \cite{silver2010monte}.  This method requires only a generative model $\mathcal{G}$  (i.e., a black box simulator of the POMDP) instead of explicit mathematical models of the motion and sensing uncertainty. During each simulation, the initial state is sampled from the initial belief state, and the generative model provides a sample of the successor state $s'$, observation $z$, and reward $r$ based on the dynamics of the environment. However, the true state of the world $W^{GT}$ is unknown.  Thus, an approximate generative model $\hat{\mathcal{G}}^g$, $\hat{\mathcal{G}}^l$ is employed for each POMDP (graph and lattice) which uses a posterior distribution of the state  based on sensor data, $p(W^{GT}; a_{0:t-1}, z_{1:t})$ (omitting superscripts $(\cdot)^g, (\cdot)^l$):
\begin{align}
    \hat{\mathcal{G}}(\cdot) = \mathcal{F}(s, a; \, p(W^{GT}; a_{1:t}, z_{1:t})) = \mathcal{F}\big[T(\cdot), O(\cdot), R(\cdot)\big]: \, \mathbb{S} \times \mathbb{A} \rightarrow \mathbb{S} \times \mathbb{Z} \times \mathbb{R}
\end{align}

As the robot moves through the environment, its sensors are used to update the pose-graph $\mathcal{PG}$ and the kinodynamic costmap $\bar{W}$.  These are sent to modules which update the global-level IRM $W^g$ and the lattice-level IRM $W^l$, as well as estimates of the generative models $\hat{\mathcal{G}}^g$ and $\hat{\mathcal{G}}^l$. For details of these modules, see the supplemental material.

\ph{A preview of POMCP Implementation Details}
\todo{Now talk a little more about implementation details of POMCP.  Heavy details can go in supplemental.}
The POMCP solver uses Monte-Carlo Tree Search over the belief space of the world. Figure~\ref{fig:belief-tree-search} shows a sample domain where the robot has covered parts of the world, shown in white. From node 2, the robot runs POMCP with the generative model $\mathcal{G}$, to generate the MCTS tree in the center of the figure. The red path (left) explores node $a$ first and then $c$, before moving onto node 5. The blue path (right) explores node $c$ first and then $a$, before moving onto 5. The solver simulates these sequences and back-propagates the rewards to output the optimal step (\texttt{move-to-$q_1$}) for the robot.

% (See Figure \ref{fig:belief-tree-search}). 

\section{Experimental Results}\label{sec:exp_results}

% CONTENTS
% - baseline
%     : may-demo
%     : GLP-MLP
%     : GLP-MLP(custom-rollout)
%         . good custom-rollout suggestions for coverage problem
% - environments
%     : toy example?
%     : mission in sim
%         . open-space (subway station)
%         . cave1
%         . cave2
%     : mission with robot
%         . caltech garden
%         . backyard
% - results
%     : toy example?
%         . average reward for different tree queries (one planning episode)
%         . average reward for different rollout policy (one planning episode)
%     : mission in sim
%         . plots
%         . path
%     : mission with robot
%         . screenshots (robot + irm)
%         . path
%         . plot (planning time, traversal distance, ...)


% SUBWAY
% => Gautam: 10depth-GLP
% => Sung: NBV in subway 2x 2run
% => David: NBV in subway 2x (1000sec~2000sec) 2runs
% => plots
% % : in station 1x size: 3runs for each already

% CAVE
% : cave from the center open space
% => 1-2pm: Kyon MLP/GLP cave.world
% => Amanda: NBV in subway 2x

% ROBOT TEST FIGURE




\subsection{Experimental Setup}

% baseline algorithms (name, approach, ...)

% system config

% experiment environment
% - sim (AWS, ...)
% - robot

% ...

\ph{setup}
We validate our proposed framework in a high-fidelity simulation. The robot system is a four-wheeled vehicle, equipped with three LiDAR range sensors returning 3D point cloud data. The environment is a maze-like cave of size $400 \times 100$m with complex terrain and topology, including narrow passages, dead-ends, open-spaces, and branches with fluctuating curvature. Together these features critically challenge the tested coverage planning schemes. The simulation tests were performed on a computer with four 3.10 GHz CPUs and 32 GB RAM.

\ph{PLGRIM setup}
The hierarchical planning framework has two POMDP planners, graph-level planner (GLP) at the high level and lattic-level planner (LLP) at the lower level.
LLP is configured to make a plan within one sec, so that the robot's path can be quickly updated and ensure the robot's safety.
In the experiments, LLP's planning horizon was of 30 steps on the local lattice, and the number of Monte Carlo simulations was 200.
On other hand, GLP is configured to make a plan within a long horizon, so that it can capture the global completeness of the coverage.
In the experiments, GLP's planning horizon was of 100 steps on the global information roadmap, and the number of Monte Carlo simulations was 200.


\ph{baseline-- Lattice-NBV} Next-Best-View on lattice

\ph{baseline-- Hier-Frontier} Hierarchical frontier-based exploration





\subsection{Simulation Results}

The autonomous exploration results are presented in this subsection. %shown in Fig.~\ref{fig:may-demo-path} and Fig.~\ref{fig:may-demo-plot}.

As shown in Fig.~\ref{fig:may-demo-path}, the robot started from the staging area on the left (white open-space area) and autonomously explored the maze-like cave over 800 m in 23 min.
Note that the ground truth environment information is not accessible by the robot.

There are many sharp corned in the environment, where the frontier-based exploration approaches may fail to detect the frontiers and abandon exploring beyond the corner.
However, LLP is able to plan for a longer horizon than frontier-based approaches with high-fidelity traversability information considered, so there was not much issues at sharp corners.
(Note that the orange segment of the path on the right in Fig.~\ref{fig:may-demo-path} did not continue into the corner since the terrain is too rough to traverse.)

There are also many dead-ends in the environment, which may cause the LLP does not see any unexplored area within the local lattice, and thus, the robot gets stuck.
However, thanks to the hierarchical planning framework, GLP successfully guided LLP so that the robot move to unexplored area in the environment, without getting stuck.

In Fig.~\ref{fig:may-demo-plot}, shows the quantitative results during the autonomous exploration mission.
% (More experimental results will come.)

\begin{figure}[t!]
  \centering
  \subfloat[]{%
    \includegraphics[width=.45\textwidth]{figures/cave_path_c2.png}
  }
  \subfloat[]{%
    \includegraphics[width=.45\textwidth]{figures/cave_metrics_c2.png}
  }
  \caption{Overlay of paths traveled by robots over time on the ground truth simulation environment of a maze-like cave.  Color indicates time at which the path was traversed, from purple at start to yellow at end.}
  \label{fig:may-demo-path}
\end{figure}
\begin{figure}[!t]
  \centering
  \includegraphics[width=.75\textwidth]{figures/subway_station_coverage.png}
  \caption{Total distance traveled (blue), total area covered (purple), and the number of sectors on the environment (red) versus  time.}
  \label{fig:may-demo-plot}
\end{figure}


\begin{figure}[t!]
  \centering
  \includegraphics[width=.6\textwidth]{figures/spot_garden_test.png}
  \caption{Hardware Test}
  \label{fig:spot-garden}
\end{figure}



\begin{figure}[!t]
  \centering
  \subfloat[start]{%
    \includegraphics[width=.5\textwidth]{IRM_Planning/figures/ours_s2/s2_0.png}
  }
  \subfloat[start]{%
    \includegraphics[width=.5\textwidth]{IRM_Planning/figures/ours_s2/s2_1.png}
  }\\
  \subfloat[start]{%
    \includegraphics[width=.5\textwidth]{IRM_Planning/figures/ours_s2/s2_3.png}
  }
  \subfloat[start]{%
    \includegraphics[width=.5\textwidth]{IRM_Planning/figures/ours_s2/s2_4.png}
  }\\
  \subfloat[start]{%
    \includegraphics[width=.5\textwidth]{IRM_Planning/figures/ours_s2/s2_5.png}
  }
  \subfloat[start]{%
    \includegraphics[width=.5\textwidth]{IRM_Planning/figures/ours_s2/s2_7.png}
  }
  \caption{Baseline in a subway station environment.}
  \label{fig:station-baseline}
\end{figure}

\ph{Hardware Evaluation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}\label{sec:conclusion}

In this work, we have developed a hierarchical framework for exploring large-scale unknown environments in a POMDP setting. 
To obtain a tractable solution we discretize the belief space into a robot and task-relevant graph structure which reduces our search space for good policies.
The hierarchical planning framework and formalization of the unified optimal policy enabled scaling up the highly complex coverage problems.
We demonstrate these capabilities in the high-fidelity dynamic simulation environment.  

The future work includes learning-based methods for graph expansion information gain estimation, and a richer incorporation of risk and time into the graph planning framework.
Another interesting venue is the extension of this framework to the multi-robot coverage problems.


\clearpage{}

\bibliography{references}  % .bib

\end{document}
